# -*- coding: utf-8 -*-
"""MSI_v1.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R1JRLRXXStXePp0eIfsg3vqiDGRXHdGV

Tytuł projektu:
#Detekcja zarysów tablic rejestracyjnych

Członkowie zespołu:
- Dominik Lamcha
- Dominik Bongowski
- Tomasz Downar-Zapolski

Import potrzebnych bibliotek
"""

!pip install --upgrade albumentations
!pip install opencv-python-headless==4.1.2.30

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.utils.data import Dataset 
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

import matplotlib.pyplot as plt

import os
import os.path
import pandas as pd

import xml.etree.ElementTree as ET
import albumentations as A
import math

import albumentations.pytorch.transforms
from torchvision.utils import draw_bounding_boxes
from tqdm.auto import tqdm
from statistics import mean

seed = 56

"""Załadowanie dysku google"""

from google.colab import drive
drive.mount('/content/drive')

"""Załadowanie datasetu:
- zdefiniuj batch_size
- zdefiniuj transformacje (np. normalizacja, augmentacja); pamiętaj, że dane muszą być zapisane w postaci tensorów
- wczytaj dataset z odpowiednimi właściwościami
- stwórz dataloader
- wczytaj bounding-boxy i klasy (target)

Dla większej przejrzystości możesz stworzyć osobną klasę o nazwie Dataset, która będzie tworzyła pary [image, target], gdzie target będzie słownikiem, w którym będą klasy, bounding-boxy czy inne informacje.

Tworzenie klasy zbioru danych
"""

class CustomImageDataset(Dataset):
   def __init__(self, img_dir, annotations_dir, transform=None):
        
        self.annotations_dir = annotations_dir
        self.img_dir = img_dir
        self.transform = transform
        self.img_label = int(1)
        self.img_paths = sorted([os.path.join(img_dir, name) for name in os.listdir(img_dir)])
        self.target_paths = sorted([os.path.join(target_dir, name) for name in os.listdir(target_dir)])

   def __len__(self):
       lenght=len(self.img_paths)
       return lenght

   def __getitem__(self, idx):
        
        image = plt.imread(self.img_paths[idx])[:,:,:3]      
                
        tree = ET.parse(self.target_paths[idx])
        
        root = tree.getroot()

        obj = 4
        bndbox = 5
        siz = 2

        target = {}
        sizes = {}
        target['boxes'] = [[int(root[obj][bndbox][i].text) for i in range(4)]]
        target["labels"] = [self.img_label]
        sizes['size'] = [[int(root[siz][i].text) for i in range(2)]]

        if self.transform:
            transformed=self.transform(image=image, bboxes=target["boxes"], category_ids=target["labels"])
            image = transformed["image"]
            target['boxes'] = transformed['bboxes']
            target["labels"] = transformed['category_ids']   

        
        return image, target, sizes

"""Augumentacje i transformacje"""

def augumentation():
  return A.Compose(
  [A.Resize(224,224),
   A.HorizontalFlip(p=0.5),
   A.MotionBlur(p=0.5),
   A.Blur(blur_limit=3, p=0.5),
   A.ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5),
   ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))

def normal_transform():
  return A.Compose(
  [A.Resize(224,224),
   ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))

"""Utworzenie dataset"""

# batch_size = 16

# img_dir = "/content/drive/MyDrive/Training_set/data/images"
# target_dir = "/content/drive/MyDrive/Training_set/data/annotations"

# transform_set=CustomImageDataset(img_dir, target_dir, augumentation())
# dataset = CustomImageDataset(img_dir, target_dir, normal_transform())

# all_sets = torch.utils.data.ConcatDataset([transform_set, dataset])


# trainset, validset = torch.utils.data.random_split(all_sets, [math.floor(len(all_sets)*0.9), len(all_sets)-math.floor(len(all_sets)*0.9)], generator=torch.Generator().manual_seed(seed))
# validset, testset = torch.utils.data.random_split(validset, [math.floor(len(validset)*0.5), len(validset)-math.floor(len(validset)*0.5)], generator=torch.Generator().manual_seed(seed))

# trainloader = DataLoader(trainset,batch_size=batch_size)
# validloader = DataLoader(validset,batch_size=batch_size)
# testloader = DataLoader(testset,batch_size=batch_size)

batch_size = 16

img_dir = "/content/drive/MyDrive/Training_set/data/images"
target_dir = "/content/drive/MyDrive/Training_set/data/annotations"

transform_set=CustomImageDataset(img_dir, target_dir, augumentation())
dataset = CustomImageDataset(img_dir, target_dir, normal_transform())

trainset, szrotset = torch.utils.data.random_split(transform_set, [math.floor(len(transform_set)*0.8), len(transform_set)-math.floor(len(transform_set)*0.8)], generator=torch.Generator().manual_seed(seed))
szrot2set, validset = torch.utils.data.random_split(dataset, [math.floor(len(dataset)*0.8), len(dataset)-math.floor(len(dataset)*0.8)], generator=torch.Generator().manual_seed(seed))
validset, testset = torch.utils.data.random_split(validset, [math.floor(len(validset)*0.5), len(validset)-math.floor(len(validset)*0.5)], generator=torch.Generator().manual_seed(seed))

# trainset = torch.utils.data.ConcatDataset([transform_set, trainset])

trainloader = DataLoader(trainset,batch_size=batch_size)
validloader = DataLoader(validset,batch_size=batch_size)
testloader = DataLoader(testset,batch_size=batch_size)

"""##Podstawowe statystyki

Liczba obrazów
"""

print(len(trainloader))
print(len(validloader))

"""Wyświetlanie obrazów:
- napisz funkcję wyświtlającą obrazy z datasetu wraz z bounding-boxami i klasami
- wyświetl przykładowe obrazy
"""

def imshow(img, target, label):
  #dodać wyswietlanie labeli, usunac nazwy osi itd.
  img = img * 255
  img = draw_bounding_boxes(img.permute(2,0,1).to(torch.uint8), target.unsqueeze(0), colors='blue',  width=3, labels = [label])
  plt.imshow(img.permute(1,2,0))
  plt.axis('off')
  plt.show()

for i, item in enumerate(testloader):
  imgs = item[0]
  #display(item[1])
  targets = item[1]['boxes'][0]
  labels = item[1]['labels'][0]
  for i in range(4):
    img = imgs[i]
    target = torch.Tensor([float(targets[j][i]) for j in range(4)])
    label = str(int(labels[i]))
    imshow(img,target,label)
    print(target)
  break

"""Wyświetl statystyki opisujące dataset"""

#Inne statystyki do wyswietlenia: ilosc bndboxow na obraz, wielkosc obrazow, wielkosc bndboxow relatywna i rzeczywista, umieszczenie bndboxow na obrazie

img_dir = "/content/drive/MyDrive/Training_set/data/images"
target_dir = "/content/drive/MyDrive/Training_set/data/annotations"

dataset=CustomImageDataset(img_dir, target_dir)
centery=[]
centerx=[]
sizex=[]
sizey=[]
img_surf = []
bbox_sizex = []
bbox_sizey = []
bbox_surf = []
bbox_relative = []

for i in range(len(dataset)):
  image_stat, target_stat, size_stat = dataset.__getitem__(i)
  target_s = target_stat['boxes'][0]
  sizex += [float(size_stat['size'][0][0])]
  sizey += [float(size_stat['size'][0][1])]
  img_surf += [sizex[i] * sizey[i]]
  centery += [float((target_s[1]+target_s[3])/2)]
  centerx += [float((target_s[0]+target_s[2])/2)]
  bbox_sizex += [float((target_s[2]-target_s[0]))]
  bbox_sizey += [float((target_s[3]-target_s[1]))]
  bbox_surf += [bbox_sizex[i] * bbox_sizey[i]]
  bbox_relative += [(bbox_surf[i] / img_surf[i])*100]

plt.scatter(sizex, sizey)
plt.xlim([0, 650])
plt.ylim([0, 550])
plt.title("Rozmiar obrazów")
plt.xlabel("Szerokość [px]")
plt.ylabel("Wysokość [px]")
plt.show()

plt.scatter(centerx, centery)
plt.xlim([0, 650])
plt.ylim([0, 550])
plt.title("Środki bounding boxów")
plt.show()

plt.hist(sizex)
plt.title("Szerokość obrazów")
plt.show()

plt.hist(sizey)
plt.title("Wysokość obrazów")
plt.show()

plt.scatter(bbox_sizex, bbox_sizey)
plt.title("Rozmiar bounding boxów")
plt.xlabel("Szerokość [px]")
plt.ylabel("Wysokość [px]")
plt.show()

plt.hist(bbox_surf)
plt.title("Powierzchnia bounding boxów")
plt.show()

plt.hist(bbox_relative)
plt.title("Powierzchnia bounding boxów względem obrazu [%]")
plt.show()

"""Zdefiniuj swoją sieć:
- stwórz klasę, w której zdefiniujesz własną sieć
lub skorzystasz z już przeuczonej
- stwórz funkcję inicjującą warstwy sieci
- pamiętaj o stworzeniu funkcji forward,
  która odpowiada za przejście danych uczących przez sieć
- stwórz obiekt 'net' tej klasy
"""

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
net = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
net.to(device)

"""Zdefiniuj funkcję celu oraz optymalizator:"""

params = [p for p in net.parameters() if p.requires_grad]
optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)

"""Zdefiniuj pętlę treningową:
- zdefiniuj liczbę epok - liczbę iteracji przejścia przez sieć
- zdefiniuj dane wejściowe w postaci [inputs, target]
- wyzeruj gradient
- podaj dane wejściowe na sieć
- oblicz loss
- dodaj optymalizację
- wyświetl statystyki dla tej epoki

"""

def give_data(item):

    imgs = item[0]
    targets_bad = item[1]['boxes'][0]
    labels = item[1]['labels'][0]
    labels = labels.unsqueeze(1)
    boxes=torch.stack(targets_bad,1)
    boxes = boxes.unsqueeze(1)

    targets = []
    images = []
    for i in range(len(imgs)):
      img = imgs[i].permute((2, 0, 1)).to(device)
      images.append(img)
      
      d = {}
      d['boxes'] = boxes[i].to(device)
      d['labels'] = labels[i].to(device)
      targets.append(d) 
    return images,targets

num_epoch = 60
target=[]
train_loss_list = []
valid_loss_list = []
for epoch in range(num_epoch):  # loop over the dataset multiple times
    print('epoch: ',epoch)
    running_loss = 0.0

    prog_bar = tqdm(trainloader, total=len(trainloader))
    for j, item in enumerate(prog_bar):
        #print('batch',j)
        torch.cuda.empty_cache()
        images,targets = give_data(item)

        # zero the parameter gradients
        optimizer.zero_grad()
        net.train()
        outputs = net(images,targets)
        #print('outputs',outputs)
        

        loss = sum(loss for loss in outputs.values())
        loss_value = loss.item()
        train_loss_list.append(loss_value)

        #print('loss_value',loss_value)
        #print('loss',loss)

        loss.backward()
        optimizer.step()

        #print statistics
        prog_bar.set_description(desc=f"Loss: {loss_value:.4f}")

    ##sprzwdzenie uczenia zbiorem walidacyjnym
    with torch.no_grad():
      for j, item in enumerate(validloader):
          valid_loss_epoch = []
          outputs = net(images,targets)
          loss = sum(loss for loss in outputs.values())
          loss_value = loss.item()
          valid_loss_epoch.append(loss_value)
          valid_loss_list.append(loss_value)
    print('Mean loss value (validset):',mean(np.array(valid_loss_epoch)))

"""Zapisz wagi modelu"""

torch.save(net.state_dict(), '/content/drive/MyDrive/Training_set/fasterrcnn_resnet{}_fpn.pth'.format(epoch))
print('Finished Training')

"""Statystyki uczenia"""

epoch_train_loss_list = [0 for i in range(len(trainloader))]
mean_train_loss_list =[[0] for i in range(int(len(train_loss_list)/len(trainloader)))]
epoch_train_loss_list[0] = train_loss_list[0]
print(epoch_train_loss_list)
print(train_loss_list)
mean_train_loss_list[0] = mean(np.array(epoch_train_loss_list))

Batch_num = np.arange(len(train_loss_list))
plt.plot(Batch_num,train_loss_list)
plt.show()

epoch_train_loss_list = [[0] for i in range(len(trainloader))]
mean_train_loss_list =[[0] for i in range(int(len(train_loss_list)/len(trainloader)))]

for j in range(num_epoch):
  for i in range(len(trainloader)):
    epoch_train_loss_list[i] = train_loss_list[i+(len(trainloader)*j)]

  mean_train_loss_list[j] = mean(np.array(epoch_train_loss_list))

min_epoch_loss = min(mean_train_loss_list)
min_epoch_loss_idx = mean_train_loss_list.index(min_epoch_loss)

print('\nNajniższa wartość średnia błędu wśród epok wynosi',min_epoch_loss,'i wystąpiła dla epoki',min_epoch_loss_idx,'\n')

Epoch_num = np.arange(num_epoch)
plt.scatter(Epoch_num,mean_train_loss_list)
plt.grid()
plt.show()

"""Wyświetl przykładowe obrazy:
- wczytaj wagi modelu
- przetestuj sieć na obrazach
- wyświetl obrazy
- wyświetl prawdziwe oznaczenia
- wyświetl predykcje wygenerowane przez sieć
"""

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
net = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

state_dict = torch.load('/content/drive/MyDrive/Training_set/fasterrcnn_resnet59_fpn.pth', map_location=device) #wczytywanie wag modelu
net.load_state_dict(state_dict)
net.to(device).eval()



def imshow_multi(img, target):
  #dodać wyswietlanie labeli, usunac nazwy osi itd.
  img = img * 255
  img = draw_bounding_boxes(img.permute(2,0,1).to(torch.uint8), target, colors='blue',  width=3)
  plt.imshow(img.permute(1,2,0))
  plt.axis('off')
  plt.show()

with torch.no_grad():
  for j, item in enumerate(testloader):
    print(j, ' batch_num')


    imgs = item[0]
    targets_bad = item[1]['boxes'][0]
    labels = item[1]['labels'][0]
    labels = labels.unsqueeze(1)
    boxes=torch.stack(targets_bad,1)
    boxes = boxes.unsqueeze(1)

    targets = []
    images = []
    for i in range(2):
      img = imgs[i].permute((2, 0, 1)).to(device)
      images.append(img)
      
      d = {}
      d['boxes'] = boxes[i]
      d['labels'] = labels[i]
      targets.append(d)

    net.eval()
    predictions = net(images)
    #print(predictions)
    for n in range(len(predictions)):
      # for nn in range(len(predictions[n]['boxes'])):
        # print(predictions[n]['boxes'][nn])
        # print(boxes[n][0])
        # iou = iou + bb_intersection_over_union(predictions[n]['boxes'][nn],boxes[n][0])

      imshow_multi(imgs[n],predictions[n]['boxes'])

"""Oblicz metryki:
- oblicz dokładność dla całego zbioru
- oblicz dokładność dla każdej z klas
- oblicz pozostałe metryki istotne dla ewaluacji detekcji
"""

def bb_intersection_over_union(boxA, boxB):
	# determine the (x, y)-coordinates of the intersection rectangle
	xA = max(boxA[0], boxB[0])
	yA = max(boxA[1], boxB[1])
	xB = min(boxA[2], boxB[2])
	yB = min(boxA[3], boxB[3])
	# compute the area of intersection rectangle
	interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
	# compute the area of both the prediction and ground-truth
	# rectangles
	boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
	boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)
	# compute the intersection over union by taking the intersection
	# area and dividing it by the sum of prediction + ground-truth
	# areas - the interesection area
	iou = interArea / float(boxAArea + boxBArea - interArea)
	# return the intersection over union value
	return iou

def intersection_over_union(predictions,boxes,TP ,FP ,FN):
  iou_list = []
  scores_list = []
  sum_iou=0

  for n in range(len(predictions)):
    tempy = []
    tempyskor = []
    GIT_ZAZNACZONE = 0
    for nn in range(len(predictions[n]['boxes'])):
      tempytemp=bb_intersection_over_union(predictions[n]['boxes'][nn],boxes[n][0])
      iou_list.append(tempytemp)
      tempyskortemp=predictions[n]['scores'][nn]
      scores_list.append(tempyskortemp)
      if tempytemp > 0.5 and tempyskortemp > 0.7 :
        GIT_ZAZNACZONE=1
      tempy.append(tempytemp)
      tempyskor.append(tempyskortemp)
    if GIT_ZAZNACZONE == 0:
      FN = FN + 1


  iou = torch.zeros(len(iou_list))
  for i in range(len(iou_list)): 
    iou[i] = iou_list[i]
    sum_iou=sum_iou+iou_list[i]
    if iou[i] > 0.5 and scores_list[i] > 0.7 :
      TP = TP +1

    if iou[i] < 0.5 and scores_list[i] > 0.7 :
      FP = FP +1

  mean_iou=sum_iou/len(iou_list)
  return iou,mean_iou,TP,FP,FN



total_prediction=[]

TP = 0
FP = 0
FN = 0
meaner_iou=0
with torch.no_grad():
  for j, item in enumerate(testloader):
    print(j, ' batch_num')
    torch.cuda.empty_cache()
    imgs = item[0]
    targets_bad = item[1]['boxes'][0]
    labels = item[1]['labels'][0]
    labels = labels.unsqueeze(1)
    boxes=torch.stack(targets_bad,1)
    boxes = boxes.unsqueeze(1)

    targets = []
    images = []
    for i in range(len(imgs)):
      img = imgs[i].permute((2, 0, 1)).to(device)
      images.append(img)
      
      d = {}
      d['boxes'] = boxes[i]
      d['labels'] = labels[i]
      targets.append(d)

    net.eval()
    predictions = net(images)
    iou,mean_iou,TP,FP,FN = intersection_over_union(predictions,boxes,TP,FP,FN)
    meaner_iou=meaner_iou+mean_iou

meanest_iou=meaner_iou/len(validloader)                                                                           
print('Mean intersection over union value for testloader:',float(meanest_iou))
print('Correct prediciton count:',TP,'\nIncorrect prediction count:',FP)
print(FN)

